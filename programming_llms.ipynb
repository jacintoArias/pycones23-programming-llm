{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyConEs Canarias 2023\n",
    "\n",
    "## Utilizando LLMs como nuevo paradigma de programacion\n",
    "\n",
    "### Jacinto Arias - Taidy Cloud\n",
    "#### jacinto.arias@taidy.cloud\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a poner en pr√°ctica los conceptos de LLMs vistos durante el taller, utilizaremos diversas librer√≠as, como openai, langchain o gptall.\n",
    "\n",
    "- Puedes ejecutar toda la libreta en un pc convencional, simplemente instala los paquetes usando tu herramienta favorita. Las celdas de GPT4All (`llm_local`) es muy probable que no funcionen en equipos windows sin configuraci√≥n adicional\n",
    "- Algunas celdas no se pueden ejecutar porque requieren servicios que no cubrimos en el taller, estar√°n marcadas como __DEMO!__\n",
    "- Algunas celdas descargan datos en la cache de tu m√°quina, acu√©rdate de limpiarla cuando termines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paquetes b√°sicos y configuraci√≥n\n",
    "\n",
    "Iremos importando la funcionalidad conforme la necesitemos.\n",
    "\n",
    "## Dotenv\n",
    "\n",
    "Usaremos python-dotenv para cargar nuestra clave de OPEN_AI como variable de entorno, deber√°s crear el fichero .env correspondiente en tu entorno y pegar ahi la clave de OpenAI\n",
    "\n",
    "__Recuerda no compartir esta clave ni subir este fichero a un repo!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Usando OpenAI desde python\n",
    "\n",
    "En esta demo cubriremos el uso b√°sico de OpenAI y su API de `completions` para trabajar con los modelos GPT desde python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion (Legacy)\n",
    "\n",
    "La API de completions nos permite generar texto de manera predictiva en funci√≥n de un input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"text-davinci-003\", \n",
    "    prompt=\"The recipe for carbonara is\",\n",
    "    temperature=1,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 02: Prompt Engineering 101\n",
    "\n",
    "En esta demo veremos varias t√©cnicas de prompting b√°sico\n",
    "\n",
    "- Zero Shot\n",
    "- Few Shot\n",
    "- Modelos entrenados como Chat\n",
    "- Otros modelos (GPT4All + Anthropic via AWS Bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt\n",
    "\n",
    "Un prompt directo que conf√≠a √∫nicamente en la capacidad de los par√°metros de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. \n",
    "You will provide instructions to the user on how to make a recipe by mentioning first ingredients, \n",
    "measures as well as instructions.\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot mejorado\n",
    "\n",
    "Siempre podemos refinar nuestro prompt acotando la tarea a realizar, por ejemplo con instrucciones sobre c√≥mo queremos que formatee la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "In your recipes you will provide information using bullet points about:\n",
    "\n",
    "- Servings: Number of servings you are measuring for\n",
    "- Ingredients: List of ingredients and measures\n",
    "- Instructions: Detailled instructions, step by step\n",
    "\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompts\n",
    "\n",
    "Los LLMs no son muy potentes a la hora de generalizar problemas y suelen pecar de dispersarse y no resolver los problemas correctos.\n",
    "\n",
    "Proporcionar ejemplos en nuestro prompt puede ayudarnos a mejorar la calidad de las respuestas. F√≠jate que el propio ejemplo puede servir para condicionar el formato de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "In your recipes you will provide information about:\n",
    "\n",
    "- Number of servings you are measuring for\n",
    "- List of ingredients and measures\n",
    "- Detailled instructions, step by step\n",
    "\n",
    "User: Give me the recipe for valencian paella\n",
    "Assistant: *Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "Los modelos de chat est√°n espec√≠ficamente entrenados (ver RLHF) para poder responder mejor a las instrucciones en los prompts. Para ello, simulan una conversaci√≥n que permite modelar mejor la interacci√≥n y la ejecuci√≥n de acciones, pero en el fondo es muy similar a lo que hacen el resto de modelos.\n",
    "\n",
    "En OpenAI son los modelos principales como GPT-3.5 y GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"\"\"\n",
    "        You are a helpful assistant that help people with recipes and dishes\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"What is the recipe for carbonara?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages\n",
    "\n",
    "El system message es un mensaje muy especial que puede utilizarse para proporcionar instrucciones, pero hay que tener cuidado, muchos modelos no hacen caso de el...\n",
    "\n",
    "Como pod√©is ver en el caso de GPT-3.5 puede condicionar totalmente el resultado.\n",
    "\n",
    "__NOTA üëπ:__ Usar un LLM para potenciar estereotipos y sesgos est√° __muy mal__ esta es solo una demo para que veais lo bien que se le da..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are a helpful assistant that help people with recipes and dishes, \n",
    "            you are an expert in italian cuisine and value and respect tradition, \n",
    "            you will describe the recipes evocating your memories about your past and your nona, \n",
    "            simulate an italian accent when typing.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for carbonara?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de conversaci√≥n + Few Shot Learning\n",
    "\n",
    "Productos como ChatGPT est√°n implementados para mejorar los flujos de conversaci√≥n de los modelos y proporcionar esa experiencia de usuario tan conocida. En la API nos toca implementarlo a nosotros concatenando los mensajes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(dict(completion[\"choices\"][0][\"message\"]))\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"So shall I put lots of cream in it?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De vuelta a Few Shot Prompting\n",
    "\n",
    "El flujo conversacional se puede utilizar para enfatizar el comportamiento del Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are a helpful assistant with expert chef knowledge. \n",
    "            You will provide instructions to the user on how to make a recipe.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for valencian paella?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"*Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for spanish potato omelette?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot + Ground Truth\n",
    "\n",
    "Si lo modelamos bien, la t√©cnica de Few Shot Prompting no deja de ser una especie de aprendizaje supervisado.\n",
    "\n",
    "En la teor√≠a hay aproximaciones que consideran estos ejemplos como parte de los par√°metros de la red e incorporan t√©cnicas de aprendizaje y optimizaci√≥n al prompt (ver anexo a la charla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "            The sentences come from customers calls to an IVR stating the intent of their call,\n",
    "            and must be routed to he corresponding department, \n",
    "            the available labels are: \"sales\", \"support\", \"billing\", \"other\"\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Buenos d√≠as, he recibido un cargo por duplicado de mi ultima factura y me gustar√≠a recibir un reembolso\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"billing\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"support\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Me gustar√≠a contratar una linea movil adicional\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"sales\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"¬øHola? Uy vaya me he equivocado.. Pepe!! ¬øPero qu√© numero me has dado?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"other\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(msg):\n",
    "    prompt = prompt_messages + [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": msg\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=prompt\n",
    "    )\n",
    "\n",
    "    return completion[\"choices\"][0][\"message\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Mi gato ha tirado el router al suelo y se ha roto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prommt_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "            The sentences come from customers calls to an IVR stating the intent of their call and must be routed \n",
    "            to he corresponding department, the available labels are: \"sales\", \"support\", \"billing\", \"other\" \n",
    "            as well as by the sentiment of the message: \"positive\", \"negative\", \"neutral\"\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Buenos d√≠as, he recibido un cargo por duplicado de mi ultima factura y me gustar√≠a recibir un reembolso\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"billing\", \"sentiment\": \"neutral\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"support\", \"sentiment\": \"negative\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Me gustar√≠a contratar una linea movil adicional\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"sales\", \"sentiment\": \"neutral\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"¬øHola? Uy vaya me he equivocado.. Pepe!! ¬øPero qu√© numero me has dado?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"billing\", \"sentiment\": \"negative\"}\"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Mi gato ha tirado el router al suelo y se ha roto!!, porfi majetes pod√©is enviarme un t√©cnico?, lo necesito con urgencia! muchas gracias!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hay vida fuera de OpenAI\n",
    "\n",
    "En la pr√°ctica todas estas t√©cnicas deber√≠an funcionar de manera gen√©rica con cualquier LLM, sin embargo el comportamiento de estos modelos puede variar en funci√≥n de cada modelo por su arquitectura, complejidad... fine tuning aplicado..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face ü§ó y modelos open\n",
    "\n",
    "Sin duda el mejor lugar para explorar, simplemente ten en cuenta temas de licencia y c√≥digo √©tico.\n",
    "\n",
    "Para muchos modelos como Falcon, necesitar√°s una infraestructura costosa (puedes usar un hyperscaler), otras alternativas te permitir√°n depurar e incluso implantar la funcionalidad de un LLM en tu propio laptop, como es el caso e las redes cuantizadas como GPT4All o llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este codigo necesita una GPU potente no ejecutar!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = pipeline(\n",
    "#    \"The ingredients of carbonara are:\",\n",
    "#     max_length=50,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "\n",
    "__ATENCION__ Este c√≥digo no funcionar√° correctamente en un entorno windows sin configurar... ver el repo de GTP4All o usa una alternativa como google collab\n",
    "\n",
    "https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT4All(\n",
    "    model_name='ggml-model-gpt4all-falcon-q4_0.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "User: Give me the recipe for valencian paella\n",
    "Assistant: *Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "    \"\"\", \n",
    "    max_tokens=300,\n",
    "    # temp=0.6,\n",
    "    # top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain ü¶ú‚õìÔ∏è\n",
    "\n",
    "Langchain es una librer√≠a que nos a a permitir abstraer el uso de LLMs y de las t√©cnicas de prompting y gesti√≥n de conocimiento m√°s avanzadas.\n",
    "\n",
    "Su documentaci√≥n es un poco ca√≥tica y debe utilizarse con cautela, pues es un proyecto muy vivo y con mucho enfoque de investigaci√≥n, pero es perfecto para aprender y prototipar, en el futuro quiz√°s sea una buena opci√≥n para productivizar este tipo de software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain + OpenAI\n",
    "\n",
    "La mayor√≠a de ejemplos que os encontrar√©is ahi fuera estar√°n basados en el uso de modelos de OpenAI con langchain, si hab√©is cargado antes vuestra API Key, este c√≥digo deber√≠a funcionaros\n",
    "\n",
    "El m√≥dulo m√°s b√°sico de langchain es el de LLMs, y permite abstraer los modelos de una manera sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"The recipe for carbonara is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt debugging\n",
    "\n",
    "Una de las cosas buenas de langchain es que implementan una gran cantidad de utilidades en cada modelo y componente que integran, como la capacidad de monitorizar el consumo de la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate([\"The recipe for carbonara is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Prompts\n",
    "\n",
    "El segundo m√≥dulo m√°s importante de Langchain es la capacidad de gestionar prompts, algunos critican que es solo una manera compleja de utilizar _string templates_ pero para otros es una aproximaci√≥n a un futuro paradigma de modelado y composici√≥n de funcionalidad, como la orientaci√≥n a objetos.\n",
    "\n",
    "Langchain nos permite definir plantillas de prompts para reutilizarlos y abstraer el contenido de los mismos, convirti√©ndolos en funciones de input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me the recipe for {dish}\"\n",
    ")\n",
    "\n",
    "prompt_template.format(dish=\"carbonara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(dish=\"carbonara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain + Ecosistema LLM\n",
    "\n",
    "Como hemos comentado, langchain abstrae el uso de diversas herramientas y modelos, esto nos permite cargar diversos modelos dentro de la plataforma de manera transparente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_local = GPT4All(model=\"ggml-model-gpt4all-falcon-q4_0.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_local.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock\n",
    "\n",
    "__SOLO DEMO__ para utilizar esta celda se requiere una cuenta de AWS configurada para poder utilizar bedrock\n",
    "\n",
    "Este es un servicio de AWS que permite acceder a modelos de otras compa√±√≠as en un formato de pago por uso, AWS ha decidido trabajar directamente en una integraci√≥n con langchain para poderlo poner en marcha.\n",
    "\n",
    "Una vez cargado el LLM, el uso de la API es completamente abstracto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm_aws = Bedrock(\n",
    "    credentials_profile_name=\"courses\",\n",
    "    region_name=\"us-east-1\",\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_aws.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Template\n",
    "\n",
    "La apuesta principal de langchain es proporcionar abstracciones para cualquier tipo de t√©cnica de prompt engineering, como por ejemplo el uso del few shot.\n",
    "\n",
    "Esto permite m√°s adelante integrar el few shot con otro tipo de t√©cnicas m√°s complejas como la selecci√≥n de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Buenos d√≠as, he recibido un cargo por duplicado de mi ultima factura y me gustar√≠a recibir un reembolso\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"billing\", \"sentiment\": \"neutral\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"support\", \"sentiment\": \"negative\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Me gustar√≠a contratar una linea movil adicional\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"sales\", \"sentiment\": \"neutral\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"¬øHola? Uy vaya me he equivocado.. Pepe!! ¬øPero qu√© numero me has dado?\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"billing\", \"sentiment\": \"negative\"}}\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\\nAnswer:\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.predict(prompt.format(input=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Chat Models\n",
    "\n",
    "La abstracci√≥n de prompts tambi√©n nos va a permitir modelar aspectos como las conversaciones con construcciones abstractas, muy importante pues no todos los modelos de Chat se comportan igual y previsiblemente en el futuro veremos todo tipo de APIs y modelos en el mercado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([[\n",
    "    SystemMessage(content=\"You are a helpful assistant that help people with recipes and dishes\"),\n",
    "    HumanMessage(content=\"What is the recipe for carbonara?\")\n",
    "]])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and prompt abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "        You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "        The sentences come from customers calls to an IVR stating the intent of their call,\n",
    "        and must be routed to he corresponding department, \n",
    "        the available labels are: \"sales\", \"support\", \"billing\", \"other\"\n",
    "    \"\"\"),\n",
    "    HumanMessage(content=\"Buenos d√≠as, he recibido un cargo por duplicado de mi ultima factura y me gustar√≠a recibir un reembolso\"),\n",
    "    AIMessage(content=\"billing\"),\n",
    "    HumanMessage(content=\"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"),\n",
    "    AIMessage(content=\"support\"),\n",
    "    HumanMessage(content=\"Me gustar√≠a contratar una linea movil adicional\"),\n",
    "    AIMessage(content=\"sales\"),\n",
    "    HumanMessage(content=\"¬øHola? Uy vaya me he equivocado.. Pepe!! ¬øPero qu√© numero me has dado?\"),\n",
    "    AIMessage(content=\"other\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = HumanMessagePromptTemplate.from_template(input_variables=[\"question\"], template=\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_messages + input_prompt.format_messages(question=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([prompt])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Chains\n",
    "\n",
    "El potencial de un LLM se multiplica exponencialmente cuando somos capaces de combinarlo con herramientas o de construir comportamientos complejos encadenando diversos prompts para que completen una tarea m√°s compleja entre todos, simplificando cada parte y focalizando el esfuerzo en cada prompt.\n",
    "\n",
    "Probablemente el tercer m√≥dulo m√°s importante de langchain es la capacidad de modelar el uso de cadenas\n",
    "\n",
    "Vamos a construir una cadena b√°sica con un solo LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What are the ingredients for {dish}\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "print(llm_chain.predict(dish=\"Valencian Paella\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "Vamos a incorporar una herramienta b√°sica, que no es un LLM, sino una manera de componer prompts y a√±adir una funcionalidad b√°sica, en este caso, devolver una salida procesada en lugar de texto en bruto.\n",
    "\n",
    "Re-implementar este comportamiento con tu propio c√≥digo es muy sencillo, langchain apuesta por la estandarizaci√≥n y la capacidad de integraci√≥n gracias a su Hub y sus integraciones\n",
    "\n",
    "- https://python.langchain.com/docs/integrations/providers\n",
    "- https://smith.langchain.com/hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "List all the ingredients for {dish}\n",
    "{parser_instructions}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"dish\"], \n",
    "    partial_variables={\"parser_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=output_parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain(\n",
    "    \"Valencian Paella\", \n",
    "    # callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√°s parsers\n",
    "\n",
    "Langchain proporciona otros parsers o simplemente ideas para integrar tus LLMs, por ejemplo, facilitando la salida estructurada de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invoice(BaseModel):\n",
    "    datetime: str = Field()\n",
    "    provider_name: str = Field()\n",
    "    provider_vat: str = Field()\n",
    "    num_items: int = Field()\n",
    "    subtotal: float = Field()\n",
    "    tax: float = Field()\n",
    "    total: float = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Invoice)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a tool that parses information from the raw text generated by scanning a physical ticket using an OCR\n",
    "\n",
    "    Extract the information following the instructions below:\n",
    "        datetime: Date and time of the transaction\n",
    "        provider_name: Date of the provider\n",
    "        provider_vat: VAT number of the provider, eg: ES12345678A\n",
    "        num_items: Number of items in the ticket\n",
    "        subtotal: Subtotal of the ticket\n",
    "        tax: Tax of the ticket\n",
    "        total: Total of the ticket\n",
    "\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    OCR Text: {text}\n",
    "\n",
    "    Result:\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm_chain.apply(\n",
    "    [\n",
    "        {\n",
    "            \"text\": \"\"\"\n",
    "            Factura Simplificada no 345456w/22\n",
    "            12/10/2023 12:34:56\n",
    "            Restaurante Benito\n",
    "\n",
    "            Calle Mayor 25, Albacete\n",
    "            54726418B\n",
    "\n",
    "            Articulos\n",
    "            Coca Cola Zero  1 2.00‚Ç¨ 2.00‚Ç¨\n",
    "            Ca√±a Peque√±a 1 1.25‚Ç¨ 1.25‚Ç¨\n",
    "            Marineras 2 3.00‚Ç¨ 6.00‚Ç¨\n",
    "            Subtotal 9.25‚Ç¨\n",
    "            IVA 21% 1.94‚Ç¨\n",
    "            Total 11.19‚Ç¨\n",
    "            \"\"\"\n",
    "         },\n",
    "        {\n",
    "            \"text\": \"\"\"\n",
    "            Gasolineras de la mancha\n",
    "            Factura Simplificada\n",
    "            Diesel A+ 0.65‚Ç¨/L 80L 52.00‚Ç¨\n",
    "\n",
    "            Iva 21 incluido\n",
    "\n",
    "            Matricula 1234ABC\n",
    "            Autov√≠a de Alicante Km134, Almansa\n",
    "            Gasoil Mancha SL 54726418B\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"text\"].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1][\"text\"].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "\n",
    "La funcionalidad m√°s potente de utilizar cadenas es la capacidad de encadenar varios LLMs para resolver una tarea compleja, especializando cada uno de los prompts.\n",
    "\n",
    "El siguiente ejemplo muestra la capacidad de modelado b√°sico de un sistema que resume un chat corporativo con capacidad de filtrado de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "    You are a helpful assistant that will summary conversations among employees that use a company chat in a public channel using one sentence and in spanish\n",
    "\n",
    "    Original Text: {conversation}\n",
    "    Summary:\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt_template = PromptTemplate(input_variables=[\"conversation\"], template=summary_template)\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain(\"\"\"\n",
    "    Juan: Me ha llamado el responsable de la empresa Compuglobalhypermeganet para que les hagamos un nuevo desarrollo sobre lo que ya hicimos el a√±o pasado\n",
    "    Marta: Hola Juan, te refieres al proyecto de instalacion de los nuevos servidores o la migraci√≥n de su sistema de facturaci√≥n\n",
    "    Juan: La migraci√≥n de su sistema de facturaci√≥n, quiren un nuevo m√≥dulo para gestionar una nueva pasarela de pago online\n",
    "    Marta: Ok, pues me pongo con ello, les env√≠o un presupuesto y les pido una reuni√≥n para aclarar los detalles, @Sara te asigno el proyecto a tu cuenta para que hagas seguimiento de la oferta\n",
    "    Sara: Ok, gracias Marta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain(\"\"\"\n",
    "    Pedro: @Antonio figura!! menuda llevabas el sabado, no se ni como has venido a currar hoy\n",
    "    Antonio: Calla tu, menuda cogorza, despu√©s de la **%&$ de semana que me dio el pesado este solo queria olvidarme\n",
    "    Pedro: Este jefe cada dia m√°s tonto\n",
    "    Alba: Para tontos vosotros que este es el canal de cotizaciones y lo esta viendo todo el mundo üòÖ\n",
    "    Pedro: Como puedo borrar los mensajes?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_template = \"\"\"\n",
    "    You are a moderation and filtering tool that will review texts containing the summary of a company conversation\n",
    "    If Original Text is related to a business topic you will return the same text, untouched, otherwise you will return OFFTOPIC\n",
    "\n",
    "    Original Text: {summary}\n",
    "    Filtered Text:\n",
    "\"\"\"\n",
    "\n",
    "moderation_prompt_template = PromptTemplate(input_variables=[\"summary\"], template=moderation_template)\n",
    "moderation_chain = LLMChain(llm=llm, prompt=moderation_prompt_template, output_key=\"filtered_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain = SequentialChain(\n",
    "    chains=[summary_chain, moderation_chain],\n",
    "    input_variables=[\"conversation\"],\n",
    "    output_variables=[\"filtered_summary\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain(\"\"\"\n",
    "    Juan: Me ha llamado el responsable de la empresa Compuglobalhypermeganet para que les hagamos un nuevo desarrollo sobre lo que ya hicimos el a√±o pasado\n",
    "    Marta: Hola Juan, te refieres al proyecto de instalacion de los nuevos servidores o la migraci√≥n de su sistema de facturaci√≥n\n",
    "    Juan: La migraci√≥n de su sistema de facturaci√≥n, quiren un nuevo m√≥dulo para gestionar una nueva pasarela de pago online\n",
    "    Marta: Ok, pues me pongo con ello, les env√≠o un presupuesto y les pido una reuni√≥n para aclarar los detalles, @Sara te asigno el proyecto a tu cuenta para que hagas seguimiento de la oferta\n",
    "    Sara: Ok, gracias Marta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain(\"\"\"\n",
    "    Pedro: @Antonio figura!! menuda llevabas el sabado, no se ni como has venido a currar hoy\n",
    "    Antonio: Calla tu, menuda cogorza, despu√©s de la **%&$ de semana que me dio el pesado este solo queria olvidarme\n",
    "    Pedro: Este jefe cada dia m√°s tonto\n",
    "    Alba: Para tontos vosotros que este es el canal de cotizaciones y lo esta viendo todo el mundo üòÖ\n",
    "    Pedro: Como puedo borrar los mensajes?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesti√≥n del conocimiento y la memoria\n",
    "\n",
    "Sin duda uno de los b√°sicos que debes a√±adir a tus aplicaciones que usen LLMs es la gesti√≥n de datos propios que no hayan sido utilizados para su entrenamiento. Este es el componente clave que te permitir√° construir aplicaciones que resuelvan problemas √∫nicos y novedosos.\n",
    "\n",
    "Para ello langchain proporciona una serie de utilidades que nos permitir√°n incorporar tecnolog√≠as como las bases de datos de vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos y conocimiento\n",
    "\n",
    "Recuerda que los LLMs est√°n entrenados en un momento del pasado, con datos finitos y que por tanto su conocimiento de la realidad es limitado.\n",
    "\n",
    "El problema es que muchos modelos tienden a inventarse (alucinar) las respuestas, y esto es algo muy peligroso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([[\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Que es la pycones, d√≥nde y cu√°ndo se celebra?\")\n",
    "]])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict(\"Que es la pycones, d√≥nde y cu√°ndo se celebra?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_local.predict(\"What is the pycones, when and where is it held?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Stuffing\n",
    "\n",
    "La t√©cnica m√°s sencilla para resolver este problema es a√±adir a nuestro prompt el conocimiento que pueda faltarle al LLM.\n",
    "\n",
    "Esto puede ser muy sencillo en el caso de tareas b√°sicas de resumen de textos, pero para aplicaciones ambiciosas y gen√©ricas puede ser insuficiente.\n",
    "\n",
    "Adem√°s incrementar notablemente el tama√±o de nuestros prompts tiene clara desventajas y limitaciones:\n",
    "\n",
    "- Estamos limitados al n√∫mero de tokens de la ventana contextual del modelo\n",
    "- Incrementamos el precio\n",
    "- Los modelos tienen a divagar y no focalizarse en los contenidos de prompts largos y vagos\n",
    "\n",
    "\n",
    "El el siguiente ejemplo utilizaremos t√©cnicas b√°sicas de extracci√≥n de datos de web para aportarle contexto a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_urls = [\n",
    "    \"https://2023.es.pycon.org/\",\n",
    "    # \"https://2023.es.pycon.org/faq/\",\n",
    "    # \"https://2023.es.pycon.org/ciudad/\",\n",
    "    # \"https://2023.es.pycon.org/viaje/\",\n",
    "    # \"https://2023.es.pycon.org/patrocinios/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    t.get_text()\n",
    "    for url in source_urls\n",
    "    for t in BeautifulSoup(requests.get(url).text).find_all('p')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_concat = \" \".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions using the following contextual information, \n",
    "if you do not know the answer respond clearly \"I Do not know\"\\n \n",
    "\n",
    "Context: {context}\n",
    "                                               \n",
    "Question: {question}\n",
    "                                               \n",
    "Answer:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(prompt_template.format(\n",
    "    context=context_concat, \n",
    "    question=\"Que es la PyConEs y donde se celebra\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation & Vector Stores\n",
    "\n",
    "La manera sofisticada de resolver el problema anterior es utilizar t√©cnicas de gesti√≥n de la informaci√≥n no estructurada.\n",
    "\n",
    "Esto implica la incorporaci√≥n de t√©cnicas novedosas de recuperaci√≥n de informaci√≥n (donde tambi√©n intervienen los LLMs). \n",
    "\n",
    "Concretamente, langchain est√° especializado en el uso de bases de datos de vectores, como chroma, FAISS o pinecone, que aprovechan la potencia de los LLMs para trabajar con embeddings.\n",
    "\n",
    "El funcionamiento b√°sico consiste en transformar el prompt en una representaci√≥n vectorial sem√°ntica (embedding) y realizar una b√∫squeda utilizando alg√∫n algoritmo como la similaridad. Esto nos abre muchas posibilidades para elegir tipos de software, como la base de datos concreta, el modelo de embedding, algoritmo de b√∫squeda as√≠ como las t√©cnicas de transformaci√≥n de prompts al buscar y recopilar la informaci√≥n.\n",
    "\n",
    "En el ejemplo siguiente vamos a usar una base de datos local usando chroma para incluir como documentos los p√°rrafos de las webs y asi limitar enormemente el tama√±o del prompt generado.\n",
    "\n",
    "Nos apoyaremos en langchain y su cadena predefinida de b√∫squeda en bases de datos contextuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [Document(page_content=c) for c in context]\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"aeropuerto\", k=5)\n",
    "for d in docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(\n",
    "    \"cuantas charlas hay en la pycones?\", \n",
    "    # callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain helpers\n",
    "\n",
    "Langchain proporciona una cantidad ingente de integraciones y componentes proporcionados por la comunidad, que aceleran el prototipado y la incorporaci√≥n de herramientas en todos los procesos de prompting, gracias a su naturaleza modular es cuesti√≥n de cada uno utilizar estos helpers o desarrollar la integraci√≥n de manera independiente.\n",
    "\n",
    "En el siguiente script haremos un \"scrap\" de todas las charlas de la pycones 2023 para tener un bot que nos permita responder a preguntas sobre el programa de la conferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycones_talks_urls = [  \n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9QSL79/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YMZVVQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PBDTVD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KHELNK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KW33VH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SWP7AZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7UAM7P/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FXYFQZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/RDNEXC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/78GAHC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/U78RSY/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YP9PL9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZZHMWW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7YT33P/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/F98UXU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FZKJSN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Z9YG88/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/LZ8FWD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KQVXVV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZQ778X/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UQUJVP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MXLC8T/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HCMMW7/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PT3LWB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PVJES3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ARSG8Q/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FPMUUQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YZ3TU3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/8FYCPY/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UVLLJE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/RXM3KK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VZMNRK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/X9H9U9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/P3YLBP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/C7FLWF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/NYSCCZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9NYMMU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/N9BWSG/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QKZNTQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EQJCFN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/3ZSEBF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XDTUDT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9F9WMA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/TGYBY3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/TMTRB9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VDLCXR/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/V9VX9M/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EKEZVZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DYL7CA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7X3PPN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EDNDH7/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KWRG7N/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KCGKWT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/GMWRLP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/D88YTV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HBCMXE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XFWAZV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MXBJHM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/BGQ8RJ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XNQUSH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/BGVWFX/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UT33TX/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HJGQLB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VFBZDS/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HKVWAA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UTWCZ3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YLX3NS/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YQ7RLM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DVCBZU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/87Y7CW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZYCPG3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YW79NH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SVLHVA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SCXJ3N/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZMHSVG/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MMAXDN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XXAXQJ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/N9ACAB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7ZEHGA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SWFPXZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HPGZA8/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JYFQBL/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/AYNPNM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UXHEWC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KFAXTU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZPKG73/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VSTZCL/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7KDMK8/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/NCJBTE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/CQ8EQD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/R9K7KT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/A3BRFE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QSLHJE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HBLX8F/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7BWZGN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JRZ8LK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZQSMMF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QUXVJQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JN7CTD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/GRWUT3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ASD8DD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/AGDKBR/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7VCRGQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9NGPJT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FSRU8J/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/LABN9C/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9K7AZQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DCUGRW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Z9KMUT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7KGKEN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YWFFQM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7ZZZ7D/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PJXQQM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Q8C3EZ/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AsyncHtmlLoader(pycones_talks_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(docs, tags_to_extract=[\"p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(docs_transformed)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    # search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 15}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\n",
    "    \"que charlas hablar√°n de web scraping? \", \n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational memory\n",
    "\n",
    "Una de las caracter√≠sticas m√°s potentes de ChatGPT es su capacidad para recordar eventos pasados durante una conversaci√≥n, si utilizamos la API hay que recrear este comportamiento a mano, pero por suerte langchain tiene distintos tipos de utilidades para gestionar la memoria durante la interacci√≥n con un LLM en una conversaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "# memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_llm, \n",
    "    memory=memory,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I live in valencia and I would like to travel by car to madrid, how much time will the trip be?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"And if i want to stop in Albacete on the way?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents y Tools\n",
    "\n",
    "Un poco experimental, pero Langchain proporciona la capacidad de crear comportamientos aut√≥nomos gracias a la combinaci√≥n de prompts reflexivos y herramientas, este framework llamado ReAct permite generar comportamientos para resolver problemas de manera aut√≥noma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, Tool, load_tools\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain import LLMMathChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"how high is mount teide?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Current Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or the current state of the world\"\n",
    "    )\n",
    "] + load_tools([\"llm-math\"], llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide compared to everest?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide compared to the tallest mountain in the world?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycones-llms-programming-XljxWO0P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
