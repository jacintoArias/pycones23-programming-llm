{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyConEs Canarias 2023\n",
    "\n",
    "## Utilizando LLMs como nuevo paradigma de programacion\n",
    "\n",
    "### Jacinto Arias - Taidy Cloud\n",
    "#### jacinto.arias@taidy.cloud\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a poner en práctica los conceptos de LLMs vistos durante el taller, utilizaremos diversas librerías, como openai, langchain o gptall.\n",
    "\n",
    "- Puedes ejecutar toda la libreta en un pc convencional, simplemente instala los paquetes usando tu herramienta favorita. Las celdas de GPT4All (`llm_local`) es muy probable que no funcionen en equipos windows sin configuración adicional\n",
    "- Algunas celdas no se pueden ejecutar porque requieren servicios que no cubrimos en el taller, estarán marcadas como __DEMO!__\n",
    "- Algunas celdas descargan datos en la cache de tu máquina, acuérdate de limpiarla cuando termines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paquetes básicos y configuración\n",
    "\n",
    "Iremos importando la funcionalidad conforme la necesitemos.\n",
    "\n",
    "## Dotenv\n",
    "\n",
    "Usaremos python-dotenv para cargar nuestra clave de OPEN_AI como variable de entorno, deberás crear el fichero .env correspondiente en tu entorno y pegar ahi la clave de OpenAI\n",
    "\n",
    "__Recuerda no compartir esta clave ni subir este fichero a un repo!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Usando OpenAI desde python\n",
    "\n",
    "En esta demo cubriremos el uso básico de OpenAI y su API de `completions` para trabajar con los modelos GPT desde python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion (Legacy)\n",
    "\n",
    "La API de completions nos permite generar texto de manera predictiva en función de un input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"text-davinci-003\", \n",
    "    prompt=\"The recipe for carbonara is\",\n",
    "    temperature=1,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 02: Prompt Engineering 101\n",
    "\n",
    "En esta demo veremos varias técnicas de prompting básico\n",
    "\n",
    "- Zero Shot\n",
    "- Few Shot\n",
    "- Modelos entrenados como Chat\n",
    "- Otros modelos (GPT4All + Anthropic via AWS Bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt\n",
    "\n",
    "Un prompt directo que confía únicamente en la capacidad de los parámetros de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. \n",
    "You will provide instructions to the user on how to make a recipe by mentioning first ingredients, \n",
    "measures as well as instructions.\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot mejorado\n",
    "\n",
    "Siempre podemos refinar nuestro prompt acotando la tarea a realizar, por ejemplo con instrucciones sobre cómo queremos que formatee la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "In your recipes you will provide information using bullet points about:\n",
    "\n",
    "- Servings: Number of servings you are measuring for\n",
    "- Ingredients: List of ingredients and measures\n",
    "- Instructions: Detailled instructions, step by step\n",
    "\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompts\n",
    "\n",
    "Los LLMs no son muy potentes a la hora de generalizar problemas y suelen pecar de dispersarse y no resolver los problemas correctos.\n",
    "\n",
    "Proporcionar ejemplos en nuestro prompt puede ayudarnos a mejorar la calidad de las respuestas. Fíjate que el propio ejemplo puede servir para condicionar el formato de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "In your recipes you will provide information about:\n",
    "\n",
    "- Number of servings you are measuring for\n",
    "- List of ingredients and measures\n",
    "- Detailled instructions, step by step\n",
    "\n",
    "User: Give me the recipe for valencian paella\n",
    "Assistant: *Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.Completion.create(\n",
    "    model=\"davinci-002\", \n",
    "    prompt=chef_prompt,\n",
    "    temperature=0.4,\n",
    "    max_tokens=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "Los modelos de chat están específicamente entrenados (ver RLHF) para poder responder mejor a las instrucciones en los prompts. Para ello, simulan una conversación que permite modelar mejor la interacción y la ejecución de acciones, pero en el fondo es muy similar a lo que hacen el resto de modelos.\n",
    "\n",
    "En OpenAI son los modelos principales como GPT-3.5 y GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"\"\"\n",
    "        You are a helpful assistant that help people with recipes and dishes\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"What is the recipe for carbonara?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages\n",
    "\n",
    "El system message es un mensaje muy especial que puede utilizarse para proporcionar instrucciones, pero hay que tener cuidado, muchos modelos no hacen caso de el...\n",
    "\n",
    "Como podéis ver en el caso de GPT-3.5 puede condicionar totalmente el resultado.\n",
    "\n",
    "__NOTA 👹:__ Usar un LLM para potenciar estereotipos y sesgos está __muy mal__ esta es solo una demo para que veais lo bien que se le da..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are a helpful assistant that help people with recipes and dishes, \n",
    "            you are an expert in italian cuisine and value and respect tradition, \n",
    "            you will describe the recipes evocating your memories about your past and your nona, \n",
    "            simulate an italian accent when typing.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for carbonara?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de conversación + Few Shot Learning\n",
    "\n",
    "Productos como ChatGPT están implementados para mejorar los flujos de conversación de los modelos y proporcionar esa experiencia de usuario tan conocida. En la API nos toca implementarlo a nosotros concatenando los mensajes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(dict(completion[\"choices\"][0][\"message\"]))\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"So shall I put lots of cream in it?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De vuelta a Few Shot Prompting\n",
    "\n",
    "El flujo conversacional se puede utilizar para enfatizar el comportamiento del Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are a helpful assistant with expert chef knowledge. \n",
    "            You will provide instructions to the user on how to make a recipe.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for valencian paella?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"*Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the recipe for spanish potato omelette?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion[\"choices\"][0][\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot + Ground Truth\n",
    "\n",
    "Si lo modelamos bien, la técnica de Few Shot Prompting no deja de ser una especie de aprendizaje supervisado.\n",
    "\n",
    "En la teoría hay aproximaciones que consideran estos ejemplos como parte de los parámetros de la red e incorporan técnicas de aprendizaje y optimización al prompt (ver anexo a la charla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "            The sentences come from customers calls to an IVR stating the intent of their call,\n",
    "            and must be routed to he corresponding department, \n",
    "            the available labels are: \"sales\", \"support\", \"billing\", \"other\"\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Buenos días, he recibido un cargo por duplicado de mi ultima factura y me gustaría recibir un reembolso\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"billing\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"support\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Me gustaría contratar una linea movil adicional\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"sales\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"¿Hola? Uy vaya me he equivocado.. Pepe!! ¿Pero qué numero me has dado?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"other\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(msg):\n",
    "    prompt = prompt_messages + [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": msg\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=prompt\n",
    "    )\n",
    "\n",
    "    return completion[\"choices\"][0][\"message\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Mi gato ha tirado el router al suelo y se ha roto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prommt_messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"\n",
    "            You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "            The sentences come from customers calls to an IVR stating the intent of their call and must be routed \n",
    "            to he corresponding department, the available labels are: \"sales\", \"support\", \"billing\", \"other\" \n",
    "            as well as by the sentiment of the message: \"positive\", \"negative\", \"neutral\"\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Buenos días, he recibido un cargo por duplicado de mi ultima factura y me gustaría recibir un reembolso\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"billing\", \"sentiment\": \"neutral\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"support\", \"sentiment\": \"negative\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Me gustaría contratar una linea movil adicional\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"sales\", \"sentiment\": \"neutral\"}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"¿Hola? Uy vaya me he equivocado.. Pepe!! ¿Pero qué numero me has dado?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"{\"department\": \"billing\", \"sentiment\": \"negative\"}\"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_message(\"Mi gato ha tirado el router al suelo y se ha roto!!, porfi majetes podéis enviarme un técnico?, lo necesito con urgencia! muchas gracias!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hay vida fuera de OpenAI\n",
    "\n",
    "En la práctica todas estas técnicas deberían funcionar de manera genérica con cualquier LLM, sin embargo el comportamiento de estos modelos puede variar en función de cada modelo por su arquitectura, complejidad... fine tuning aplicado..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face 🤗 y modelos open\n",
    "\n",
    "Sin duda el mejor lugar para explorar, simplemente ten en cuenta temas de licencia y código ético.\n",
    "\n",
    "Para muchos modelos como Falcon, necesitarás una infraestructura costosa (puedes usar un hyperscaler), otras alternativas te permitirán depurar e incluso implantar la funcionalidad de un LLM en tu propio laptop, como es el caso e las redes cuantizadas como GPT4All o llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este codigo necesita una GPU potente no ejecutar!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = pipeline(\n",
    "#    \"The ingredients of carbonara are:\",\n",
    "#     max_length=50,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "\n",
    "__ATENCION__ Este código no funcionará correctamente en un entorno windows sin configurar... ver el repo de GTP4All o usa una alternativa como google collab\n",
    "\n",
    "https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT4All(\n",
    "    model_name='ggml-model-gpt4all-falcon-q4_0.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    \"\"\"\n",
    "You are a helpful assistant with expert chef knowledge. You will provide instructions to the user on how to make a recipe.\n",
    "\n",
    "User: Give me the recipe for valencian paella\n",
    "Assistant: *Servings:* 6 people\n",
    "\n",
    "*Ingredients:*\n",
    "- 1/2 cup olive oil\n",
    "- 500g chicken, cut into pieces\n",
    "- 500g rabbit, cut into pieces\n",
    "- 200g green beans\n",
    "- 100g butter Beans\n",
    "- saffron and paprika season\n",
    "- A tbsp of fresh grated tomato\n",
    "- salt and pepper\n",
    "- 400g rice\n",
    "- 3 parts of water per rice\n",
    "\n",
    "*Steps:*\n",
    "- Heat the paella pan with the olive oil\n",
    "- Season and fry the chicken and rabbit until golden brown (40m)\n",
    "- Add the green beans and butter beans\n",
    "- Add the tomato saffron and paprika and cook for 5m\n",
    "- Add the water and bring to boil for 20m\n",
    "- Add the rice and cook for another 20m\n",
    "\n",
    "User: Give me the recipe for spanish potato omelette\n",
    "Assistant:\n",
    "    \"\"\", \n",
    "    max_tokens=300,\n",
    "    # temp=0.6,\n",
    "    # top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain 🦜⛓️\n",
    "\n",
    "Langchain es una librería que nos a a permitir abstraer el uso de LLMs y de las técnicas de prompting y gestión de conocimiento más avanzadas.\n",
    "\n",
    "Su documentación es un poco caótica y debe utilizarse con cautela, pues es un proyecto muy vivo y con mucho enfoque de investigación, pero es perfecto para aprender y prototipar, en el futuro quizás sea una buena opción para productivizar este tipo de software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain + OpenAI\n",
    "\n",
    "La mayoría de ejemplos que os encontraréis ahi fuera estarán basados en el uso de modelos de OpenAI con langchain, si habéis cargado antes vuestra API Key, este código debería funcionaros\n",
    "\n",
    "El módulo más básico de langchain es el de LLMs, y permite abstraer los modelos de una manera sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"The recipe for carbonara is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt debugging\n",
    "\n",
    "Una de las cosas buenas de langchain es que implementan una gran cantidad de utilidades en cada modelo y componente que integran, como la capacidad de monitorizar el consumo de la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate([\"The recipe for carbonara is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Prompts\n",
    "\n",
    "El segundo módulo más importante de Langchain es la capacidad de gestionar prompts, algunos critican que es solo una manera compleja de utilizar _string templates_ pero para otros es una aproximación a un futuro paradigma de modelado y composición de funcionalidad, como la orientación a objetos.\n",
    "\n",
    "Langchain nos permite definir plantillas de prompts para reutilizarlos y abstraer el contenido de los mismos, convirtiéndolos en funciones de input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me the recipe for {dish}\"\n",
    ")\n",
    "\n",
    "prompt_template.format(dish=\"carbonara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(dish=\"carbonara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain + Ecosistema LLM\n",
    "\n",
    "Como hemos comentado, langchain abstrae el uso de diversas herramientas y modelos, esto nos permite cargar diversos modelos dentro de la plataforma de manera transparente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_local = GPT4All(model=\"ggml-model-gpt4all-falcon-q4_0.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_local.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock\n",
    "\n",
    "__SOLO DEMO__ para utilizar esta celda se requiere una cuenta de AWS configurada para poder utilizar bedrock\n",
    "\n",
    "Este es un servicio de AWS que permite acceder a modelos de otras compañías en un formato de pago por uso, AWS ha decidido trabajar directamente en una integración con langchain para poderlo poner en marcha.\n",
    "\n",
    "Una vez cargado el LLM, el uso de la API es completamente abstracto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm_aws = Bedrock(\n",
    "    credentials_profile_name=\"courses\",\n",
    "    region_name=\"us-east-1\",\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_aws.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Template\n",
    "\n",
    "La apuesta principal de langchain es proporcionar abstracciones para cualquier tipo de técnica de prompt engineering, como por ejemplo el uso del few shot.\n",
    "\n",
    "Esto permite más adelante integrar el few shot con otro tipo de técnicas más complejas como la selección de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Buenos días, he recibido un cargo por duplicado de mi ultima factura y me gustaría recibir un reembolso\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"billing\", \"sentiment\": \"neutral\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"support\", \"sentiment\": \"negative\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Me gustaría contratar una linea movil adicional\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"sales\", \"sentiment\": \"neutral\"}}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"¿Hola? Uy vaya me he equivocado.. Pepe!! ¿Pero qué numero me has dado?\",\n",
    "        \"answer\": \"\"\"{{\"department\": \"billing\", \"sentiment\": \"negative\"}}\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\\nAnswer:\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.predict(prompt.format(input=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web, que mal funciona\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Chat Models\n",
    "\n",
    "La abstracción de prompts también nos va a permitir modelar aspectos como las conversaciones con construcciones abstractas, muy importante pues no todos los modelos de Chat se comportan igual y previsiblemente en el futuro veremos todo tipo de APIs y modelos en el mercado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([[\n",
    "    SystemMessage(content=\"You are a helpful assistant that help people with recipes and dishes\"),\n",
    "    HumanMessage(content=\"What is the recipe for carbonara?\")\n",
    "]])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and prompt abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "        You are an text classification tool that classifies sentences given the content for a telco company. \n",
    "        The sentences come from customers calls to an IVR stating the intent of their call,\n",
    "        and must be routed to he corresponding department, \n",
    "        the available labels are: \"sales\", \"support\", \"billing\", \"other\"\n",
    "    \"\"\"),\n",
    "    HumanMessage(content=\"Buenos días, he recibido un cargo por duplicado de mi ultima factura y me gustaría recibir un reembolso\"),\n",
    "    AIMessage(content=\"billing\"),\n",
    "    HumanMessage(content=\"Mi linea movil no funciona y es la tercera vez que llamo, estoy desesperada, necesito que me atiendan ya\"),\n",
    "    AIMessage(content=\"support\"),\n",
    "    HumanMessage(content=\"Me gustaría contratar una linea movil adicional\"),\n",
    "    AIMessage(content=\"sales\"),\n",
    "    HumanMessage(content=\"¿Hola? Uy vaya me he equivocado.. Pepe!! ¿Pero qué numero me has dado?\"),\n",
    "    AIMessage(content=\"other\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = HumanMessagePromptTemplate.from_template(input_variables=[\"question\"], template=\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_messages + input_prompt.format_messages(question=\"Hola? No encuentro la manera de descargar mi ultima factura de vuestra web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([prompt])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Chains\n",
    "\n",
    "El potencial de un LLM se multiplica exponencialmente cuando somos capaces de combinarlo con herramientas o de construir comportamientos complejos encadenando diversos prompts para que completen una tarea más compleja entre todos, simplificando cada parte y focalizando el esfuerzo en cada prompt.\n",
    "\n",
    "Probablemente el tercer módulo más importante de langchain es la capacidad de modelar el uso de cadenas\n",
    "\n",
    "Vamos a construir una cadena básica con un solo LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What are the ingredients for {dish}\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "print(llm_chain.predict(dish=\"Valencian Paella\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "Vamos a incorporar una herramienta básica, que no es un LLM, sino una manera de componer prompts y añadir una funcionalidad básica, en este caso, devolver una salida procesada en lugar de texto en bruto.\n",
    "\n",
    "Re-implementar este comportamiento con tu propio código es muy sencillo, langchain apuesta por la estandarización y la capacidad de integración gracias a su Hub y sus integraciones\n",
    "\n",
    "- https://python.langchain.com/docs/integrations/providers\n",
    "- https://smith.langchain.com/hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "List all the ingredients for {dish}\n",
    "{parser_instructions}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"dish\"], \n",
    "    partial_variables={\"parser_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=output_parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain(\n",
    "    \"Valencian Paella\", \n",
    "    # callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más parsers\n",
    "\n",
    "Langchain proporciona otros parsers o simplemente ideas para integrar tus LLMs, por ejemplo, facilitando la salida estructurada de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invoice(BaseModel):\n",
    "    datetime: str = Field()\n",
    "    provider_name: str = Field()\n",
    "    provider_vat: str = Field()\n",
    "    num_items: int = Field()\n",
    "    subtotal: float = Field()\n",
    "    tax: float = Field()\n",
    "    total: float = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Invoice)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a tool that parses information from the raw text generated by scanning a physical ticket using an OCR\n",
    "\n",
    "    Extract the information following the instructions below:\n",
    "        datetime: Date and time of the transaction\n",
    "        provider_name: Date of the provider\n",
    "        provider_vat: VAT number of the provider, eg: ES12345678A\n",
    "        num_items: Number of items in the ticket\n",
    "        subtotal: Subtotal of the ticket\n",
    "        tax: Tax of the ticket\n",
    "        total: Total of the ticket\n",
    "\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    OCR Text: {text}\n",
    "\n",
    "    Result:\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm_chain.apply(\n",
    "    [\n",
    "        {\n",
    "            \"text\": \"\"\"\n",
    "            Factura Simplificada no 345456w/22\n",
    "            12/10/2023 12:34:56\n",
    "            Restaurante Benito\n",
    "\n",
    "            Calle Mayor 25, Albacete\n",
    "            54726418B\n",
    "\n",
    "            Articulos\n",
    "            Coca Cola Zero  1 2.00€ 2.00€\n",
    "            Caña Pequeña 1 1.25€ 1.25€\n",
    "            Marineras 2 3.00€ 6.00€\n",
    "            Subtotal 9.25€\n",
    "            IVA 21% 1.94€\n",
    "            Total 11.19€\n",
    "            \"\"\"\n",
    "         },\n",
    "        {\n",
    "            \"text\": \"\"\"\n",
    "            Gasolineras de la mancha\n",
    "            Factura Simplificada\n",
    "            Diesel A+ 0.65€/L 80L 52.00€\n",
    "\n",
    "            Iva 21 incluido\n",
    "\n",
    "            Matricula 1234ABC\n",
    "            Autovía de Alicante Km134, Almansa\n",
    "            Gasoil Mancha SL 54726418B\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"text\"].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1][\"text\"].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "\n",
    "La funcionalidad más potente de utilizar cadenas es la capacidad de encadenar varios LLMs para resolver una tarea compleja, especializando cada uno de los prompts.\n",
    "\n",
    "El siguiente ejemplo muestra la capacidad de modelado básico de un sistema que resume un chat corporativo con capacidad de filtrado de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "    You are a helpful assistant that will summary conversations among employees that use a company chat in a public channel using one sentence and in spanish\n",
    "\n",
    "    Original Text: {conversation}\n",
    "    Summary:\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt_template = PromptTemplate(input_variables=[\"conversation\"], template=summary_template)\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain(\"\"\"\n",
    "    Juan: Me ha llamado el responsable de la empresa Compuglobalhypermeganet para que les hagamos un nuevo desarrollo sobre lo que ya hicimos el año pasado\n",
    "    Marta: Hola Juan, te refieres al proyecto de instalacion de los nuevos servidores o la migración de su sistema de facturación\n",
    "    Juan: La migración de su sistema de facturación, quiren un nuevo módulo para gestionar una nueva pasarela de pago online\n",
    "    Marta: Ok, pues me pongo con ello, les envío un presupuesto y les pido una reunión para aclarar los detalles, @Sara te asigno el proyecto a tu cuenta para que hagas seguimiento de la oferta\n",
    "    Sara: Ok, gracias Marta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain(\"\"\"\n",
    "    Pedro: @Antonio figura!! menuda llevabas el sabado, no se ni como has venido a currar hoy\n",
    "    Antonio: Calla tu, menuda cogorza, después de la **%&$ de semana que me dio el pesado este solo queria olvidarme\n",
    "    Pedro: Este jefe cada dia más tonto\n",
    "    Alba: Para tontos vosotros que este es el canal de cotizaciones y lo esta viendo todo el mundo 😅\n",
    "    Pedro: Como puedo borrar los mensajes?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_template = \"\"\"\n",
    "    You are a moderation and filtering tool that will review texts containing the summary of a company conversation\n",
    "    If Original Text is related to a business topic you will return the same text, untouched, otherwise you will return OFFTOPIC\n",
    "\n",
    "    Original Text: {summary}\n",
    "    Filtered Text:\n",
    "\"\"\"\n",
    "\n",
    "moderation_prompt_template = PromptTemplate(input_variables=[\"summary\"], template=moderation_template)\n",
    "moderation_chain = LLMChain(llm=llm, prompt=moderation_prompt_template, output_key=\"filtered_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain = SequentialChain(\n",
    "    chains=[summary_chain, moderation_chain],\n",
    "    input_variables=[\"conversation\"],\n",
    "    output_variables=[\"filtered_summary\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain(\"\"\"\n",
    "    Juan: Me ha llamado el responsable de la empresa Compuglobalhypermeganet para que les hagamos un nuevo desarrollo sobre lo que ya hicimos el año pasado\n",
    "    Marta: Hola Juan, te refieres al proyecto de instalacion de los nuevos servidores o la migración de su sistema de facturación\n",
    "    Juan: La migración de su sistema de facturación, quiren un nuevo módulo para gestionar una nueva pasarela de pago online\n",
    "    Marta: Ok, pues me pongo con ello, les envío un presupuesto y les pido una reunión para aclarar los detalles, @Sara te asigno el proyecto a tu cuenta para que hagas seguimiento de la oferta\n",
    "    Sara: Ok, gracias Marta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_chain(\"\"\"\n",
    "    Pedro: @Antonio figura!! menuda llevabas el sabado, no se ni como has venido a currar hoy\n",
    "    Antonio: Calla tu, menuda cogorza, después de la **%&$ de semana que me dio el pesado este solo queria olvidarme\n",
    "    Pedro: Este jefe cada dia más tonto\n",
    "    Alba: Para tontos vosotros que este es el canal de cotizaciones y lo esta viendo todo el mundo 😅\n",
    "    Pedro: Como puedo borrar los mensajes?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestión del conocimiento y la memoria\n",
    "\n",
    "Sin duda uno de los básicos que debes añadir a tus aplicaciones que usen LLMs es la gestión de datos propios que no hayan sido utilizados para su entrenamiento. Este es el componente clave que te permitirá construir aplicaciones que resuelvan problemas únicos y novedosos.\n",
    "\n",
    "Para ello langchain proporciona una serie de utilidades que nos permitirán incorporar tecnologías como las bases de datos de vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos y conocimiento\n",
    "\n",
    "Recuerda que los LLMs están entrenados en un momento del pasado, con datos finitos y que por tanto su conocimiento de la realidad es limitado.\n",
    "\n",
    "El problema es que muchos modelos tienden a inventarse (alucinar) las respuestas, y esto es algo muy peligroso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([[\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Que es la pycones, dónde y cuándo se celebra?\")\n",
    "]])\n",
    "\n",
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict(\"Que es la pycones, dónde y cuándo se celebra?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_local.predict(\"What is the pycones, when and where is it held?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Stuffing\n",
    "\n",
    "La técnica más sencilla para resolver este problema es añadir a nuestro prompt el conocimiento que pueda faltarle al LLM.\n",
    "\n",
    "Esto puede ser muy sencillo en el caso de tareas básicas de resumen de textos, pero para aplicaciones ambiciosas y genéricas puede ser insuficiente.\n",
    "\n",
    "Además incrementar notablemente el tamaño de nuestros prompts tiene clara desventajas y limitaciones:\n",
    "\n",
    "- Estamos limitados al número de tokens de la ventana contextual del modelo\n",
    "- Incrementamos el precio\n",
    "- Los modelos tienen a divagar y no focalizarse en los contenidos de prompts largos y vagos\n",
    "\n",
    "\n",
    "El el siguiente ejemplo utilizaremos técnicas básicas de extracción de datos de web para aportarle contexto a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_urls = [\n",
    "    \"https://2023.es.pycon.org/\",\n",
    "    # \"https://2023.es.pycon.org/faq/\",\n",
    "    # \"https://2023.es.pycon.org/ciudad/\",\n",
    "    # \"https://2023.es.pycon.org/viaje/\",\n",
    "    # \"https://2023.es.pycon.org/patrocinios/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    t.get_text()\n",
    "    for url in source_urls\n",
    "    for t in BeautifulSoup(requests.get(url).text).find_all('p')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_concat = \" \".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions using the following contextual information, \n",
    "if you do not know the answer respond clearly \"I Do not know\"\\n \n",
    "\n",
    "Context: {context}\n",
    "                                               \n",
    "Question: {question}\n",
    "                                               \n",
    "Answer:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(prompt_template.format(\n",
    "    context=context_concat, \n",
    "    question=\"Que es la PyConEs y donde se celebra\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation & Vector Stores\n",
    "\n",
    "La manera sofisticada de resolver el problema anterior es utilizar técnicas de gestión de la información no estructurada.\n",
    "\n",
    "Esto implica la incorporación de técnicas novedosas de recuperación de información (donde también intervienen los LLMs). \n",
    "\n",
    "Concretamente, langchain está especializado en el uso de bases de datos de vectores, como chroma, FAISS o pinecone, que aprovechan la potencia de los LLMs para trabajar con embeddings.\n",
    "\n",
    "El funcionamiento básico consiste en transformar el prompt en una representación vectorial semántica (embedding) y realizar una búsqueda utilizando algún algoritmo como la similaridad. Esto nos abre muchas posibilidades para elegir tipos de software, como la base de datos concreta, el modelo de embedding, algoritmo de búsqueda así como las técnicas de transformación de prompts al buscar y recopilar la información.\n",
    "\n",
    "En el ejemplo siguiente vamos a usar una base de datos local usando chroma para incluir como documentos los párrafos de las webs y asi limitar enormemente el tamaño del prompt generado.\n",
    "\n",
    "Nos apoyaremos en langchain y su cadena predefinida de búsqueda en bases de datos contextuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [Document(page_content=c) for c in context]\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"aeropuerto\", k=5)\n",
    "for d in docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(\n",
    "    \"cuantas charlas hay en la pycones?\", \n",
    "    # callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain helpers\n",
    "\n",
    "Langchain proporciona una cantidad ingente de integraciones y componentes proporcionados por la comunidad, que aceleran el prototipado y la incorporación de herramientas en todos los procesos de prompting, gracias a su naturaleza modular es cuestión de cada uno utilizar estos helpers o desarrollar la integración de manera independiente.\n",
    "\n",
    "En el siguiente script haremos un \"scrap\" de todas las charlas de la pycones 2023 para tener un bot que nos permita responder a preguntas sobre el programa de la conferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycones_talks_urls = [  \n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9QSL79/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YMZVVQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PBDTVD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KHELNK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KW33VH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SWP7AZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7UAM7P/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FXYFQZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/RDNEXC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/78GAHC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/U78RSY/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YP9PL9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZZHMWW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7YT33P/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/F98UXU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FZKJSN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Z9YG88/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/LZ8FWD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KQVXVV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZQ778X/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UQUJVP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MXLC8T/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HCMMW7/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PT3LWB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PVJES3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ARSG8Q/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FPMUUQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YZ3TU3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/8FYCPY/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UVLLJE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/RXM3KK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VZMNRK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/X9H9U9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/P3YLBP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/C7FLWF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/NYSCCZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9NYMMU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/N9BWSG/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QKZNTQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EQJCFN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/3ZSEBF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XDTUDT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9F9WMA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/TGYBY3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/TMTRB9/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VDLCXR/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/V9VX9M/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EKEZVZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DYL7CA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7X3PPN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/EDNDH7/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KWRG7N/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KCGKWT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/GMWRLP/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/D88YTV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HBCMXE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XFWAZV/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MXBJHM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/BGQ8RJ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XNQUSH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/BGVWFX/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UT33TX/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HJGQLB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VFBZDS/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HKVWAA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UTWCZ3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YLX3NS/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YQ7RLM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DVCBZU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/87Y7CW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZYCPG3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YW79NH/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SVLHVA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SCXJ3N/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZMHSVG/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/MMAXDN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/XXAXQJ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/N9ACAB/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7ZEHGA/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/SWFPXZ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HPGZA8/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JYFQBL/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/AYNPNM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/UXHEWC/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/KFAXTU/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZPKG73/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/VSTZCL/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7KDMK8/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/NCJBTE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/CQ8EQD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/R9K7KT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/A3BRFE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QSLHJE/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/HBLX8F/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7BWZGN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JRZ8LK/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ZQSMMF/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/QUXVJQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/JN7CTD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/GRWUT3/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/ASD8DD/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/AGDKBR/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7VCRGQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9NGPJT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/FSRU8J/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/LABN9C/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/9K7AZQ/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/DCUGRW/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Z9KMUT/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7KGKEN/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/YWFFQM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/7ZZZ7D/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/PJXQQM/',\n",
    "    'https://charlas.2023.es.pycon.org/pycones-2023/talk/Q8C3EZ/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AsyncHtmlLoader(pycones_talks_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(docs, tags_to_extract=[\"p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(docs_transformed)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    # search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 15}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\n",
    "    \"que charlas hablarán de web scraping? \", \n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational memory\n",
    "\n",
    "Una de las características más potentes de ChatGPT es su capacidad para recordar eventos pasados durante una conversación, si utilizamos la API hay que recrear este comportamiento a mano, pero por suerte langchain tiene distintos tipos de utilidades para gestionar la memoria durante la interacción con un LLM en una conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "# memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_llm, \n",
    "    memory=memory,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I live in valencia and I would like to travel by car to madrid, how much time will the trip be?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"And if i want to stop in Albacete on the way?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents y Tools\n",
    "\n",
    "Un poco experimental, pero Langchain proporciona la capacidad de crear comportamientos autónomos gracias a la combinación de prompts reflexivos y herramientas, este framework llamado ReAct permite generar comportamientos para resolver problemas de manera autónoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, Tool, load_tools\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain import LLMMathChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"how high is mount teide?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Current Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or the current state of the world\"\n",
    "    )\n",
    "] + load_tools([\"llm-math\"], llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide compared to everest?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how high is mount teide compared to the tallest mountain in the world?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycones-llms-programming-XljxWO0P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
